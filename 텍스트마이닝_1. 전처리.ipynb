{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ebcf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /data/ydkim/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package webtext to /data/ydkim/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n",
      "[nltk_data] Downloading package wordnet to /data/ydkim/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /data/ydkim/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /data/ydkim/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 nltk 라이브러리 다운로드\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe4547",
   "metadata": {},
   "source": [
    "# 토근화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08861134",
   "metadata": {},
   "source": [
    "## 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c0789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 주어진 텍스트를 문장 단위로 토큰화. 주로 . ! ? 등을 이용\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e84bc",
   "metadata": {},
   "source": [
    "## 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf364c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 주어진 text를 word 단위로 tokenize함\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110aa896",
   "metadata": {},
   "source": [
    "위 결과를 보면 마침표나 느낌표가 별도의 단어로 분리되어 있으며, It's는 It과 's로 분리됨을 볼 수 있다.NLTK의 WordPunctTokenizer를 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e93576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d66e4",
   "metadata": {},
   "source": [
    "word_tokenize와 달리 WordPunctTokenizer는 It's를 It, ', s의 세 토큰으로 분리하는 것을 볼 수 있다. 따라서 사용자는 토크나이저의 특성을 파악하고 자신의 목적에 맞는 토크나이저를 선택할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54f0f5",
   "metadata": {},
   "source": [
    "## 노이즈와 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa9fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "english_stops = set(stopwords.words('english')) # 반복이 되지 않도록 set으로 변환\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower()) # word_tokenize로 토큰화\n",
    "\n",
    "# stopwords를 제외한 단어들만으로 list를 생성\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1754627",
   "metadata": {},
   "source": [
    "위 결과를 보면, I, couldn't, to가 제거된 것을 볼 수 있다. 즉 이 세 단어는 불용어 사전에 포함돼 있다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61687195",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27258dd1",
   "metadata": {},
   "source": [
    "정규화는 같은 의미를 가진 동일한 단어이면서 다른 형태로 쓰여진 단어들을 통힐해 표준 단어로 만드는 작업을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b9cbe",
   "metadata": {},
   "source": [
    "## 어간 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16acdd",
   "metadata": {},
   "source": [
    "어간 추출은 '어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해 내는 작업'을 말한다.  \n",
    "영어와 우리말은 원리와 구조가 다르므로 어간 추출도 달라질 수밖에 없다.  \n",
    "영어의 경우에는 명사가 복수형으로 기술된 것을 단수형으로 바꾸는 작업도 어간 추출에 포함된다.  \n",
    "영어에 대한 어간추출 알고리즘으로틑 포터 스테머, 랭카스터 스테머 등이 잘 알려져 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fde0e",
   "metadata": {},
   "source": [
    "다음은 NLTK의 포터 스테머를 사용해 어간 추출을 하는 예다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe5f9ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4969d50",
   "metadata": {},
   "source": [
    "위와 같이 스테머 알고리즘은 단어가 변형되는 규칙을 이용해 원형을 찾으므로, 그 결과가 항상 사전에 있는 올바른 단어가 되지는 않는다.  \n",
    "중요한 것은 포터 스테머를 쓰면 모든 단어가 같은 규칙에 따라 변환된다는 것이다. 즉 변환된 단어가 올바른 단어가 아니더라도, 동일한 형태로 변환됐으므로 분석의 의도를 충족시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3eb771",
   "metadata": {},
   "source": [
    "다음은 토큰화와 결합해 어간 추출을 하는 예이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0098d66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para) # 토큰화 실행\n",
    "print(tokens)\n",
    "result = [stemmer.stem(token) for token in tokens] # 모든 토큰에 대해 스테밍 실행\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910fdb0",
   "metadata": {},
   "source": [
    "## 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313e3dd",
   "metadata": {},
   "source": [
    "표제어 추출은 단어의 기본형으로 변환한다는 뜻이다.어간과 단어의 기본형의 차이는 사전에 나오는 단어인지 아닌지의 차이로 구분할 수 있다.  \n",
    "영어에 대한 표제어 추출기로 유명한 어휘 데이터베이스인 WordNet을 이용한 WordNet lemmatizer가 잘 알려져 있다.  \n",
    "다음은 표제어를 추출하는 예를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6174f270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to /data/ydkim/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /data/ydkim/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2df8a7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookery\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v')) # 품사를 지정\n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d898f56c",
   "metadata": {},
   "source": [
    "# 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4116d",
   "metadata": {},
   "source": [
    "품사 태깅은 형태소에 대해 품사를 파악해 부착하는 작업을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffe5ca",
   "metadata": {},
   "source": [
    "## NLTK를 활용한 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15bf62",
   "metadata": {},
   "source": [
    "영어로 된 텍스트에 대해 품사 태깅을 하고자 한다면 지금까지와 마찬가지로 NLTK를 쓰는 것이 가장 편하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a51cd",
   "metadata": {},
   "source": [
    "nltk.pos_tag()은 토큰화된 결과에 대해 품사를 태깅해 (단어, 품사)로 구성된 튜플의 리스트로 품사 태깅 결과를 반환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb5a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\")\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce0ba4",
   "metadata": {},
   "source": [
    "### 원하는 품사의 단어들만 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5781e9d8",
   "metadata": {},
   "source": [
    "자연어 분석을 할 때에는 상황에 따라 명사만 필요하거나, 특정 품사들만 사용할 때가 있다. 이 경우에는 다음과 같은 방법으로 원하는 품사들을 골라낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d76a52fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "my_tag_set = ['NN', 'VB', 'JJ']\n",
    "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "print(my_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aab8a0",
   "metadata": {},
   "source": [
    "## 한글 형태소 분석과 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38858304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('프로그램', 'Noun'), ('설치', 'Noun'), ('를', 'Josa'), ('했습니다', 'Verb'), ('.', 'Punctuation'), ('뭐', 'Noun'), ('하나', 'Noun'), ('쉬운', 'Adjective'), ('게', 'Noun'), ('없네', 'Adjective')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "print(okt.pos('프로그램 설치를 했습니다. 뭐 하나 쉬운 게 없네'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9688c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
