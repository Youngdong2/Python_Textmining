{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004086c0",
   "metadata": {},
   "source": [
    "## BOW 기반의 카운트 벡터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933edf9",
   "metadata": {},
   "source": [
    "BOW의 실습을 위해 대상 말뭉치로 NLTK가 제공하는 영화 리뷰를 사용하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87455924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#review count : 2000\n",
      "#samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "#id of the first review: neg/cv000_29416.txt\n",
      "#first review content:\n",
      " plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "w\n",
      "#sentence tokenization result: [['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.']]\n",
      "#word tokenization result: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /data/ydkim/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print('#review count :', len(movie_reviews.fileids())) # 영화 리뷰 문서의 id를 반환\n",
    "print('#samples of file ids:', movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
    "\n",
    "fileid = movie_reviews.fileids()[0] # 첫번째 문서의 id를 반환\n",
    "print('#id of the first review:', fileid)\n",
    "\n",
    "# 첫번째 문서의 내용을 200자까지만 출력\n",
    "print('#first review content:\\n', movie_reviews.raw(fileid)[:200])\n",
    "\n",
    "# 첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장\n",
    "print('#sentence tokenization result:', movie_reviews.sents(fileid)[:2])\n",
    "\n",
    "# 첫번째 문서를 word tokenize한 결과 중 앞 20개 단어\n",
    "print('#word tokenization result:', movie_reviews.words(fileid)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e09813",
   "metadata": {},
   "source": [
    "먼저 각 문서에 대한 토큰화 결과들로 리스트를 만들기 위해 아래 코드를 실행한다.  \n",
    "fileids()를 이용해 모든 문서의 id를 가져오고 각 id들에 대해 words()로 토큰화 결과를 가져와 리스트를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4636dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch']\n"
     ]
    }
   ],
   "source": [
    "documents = [list(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()]\n",
    "print(documents[0][:50]) # 첫번째 문서의 앞 50개 단어 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c935f70",
   "metadata": {},
   "source": [
    "이제 특성 집합을 만들기 위해 딕셔너리를 써서 단어별로 말뭉치 전체에서의 빈도를 계산하고, 빈도가 높은 단어부터 정렬한 후에 빈도수 상위 20개의 단어를 출력한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8305f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of ',': 77717, count of 'the': 76529, count of '.': 65876, count of 'a': 38106, count of 'and': 35576, count of 'of': 34123, count of 'to': 31937, count of ''': 30585, count of 'is': 25195, count of 'in': 21822, "
     ]
    }
   ],
   "source": [
    "word_count = {}\n",
    "for text in documents:\n",
    "    for word in text:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "        \n",
    "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
    "for word in sorted_features[:10]:\n",
    "    print(f\"count of '{word}': {word_count[word]}\", end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46226a22",
   "metadata": {},
   "source": [
    "위 결과를 보면 ',', 'the', 'a'와 같이 의미적으로 쓸모없는 단어만 빈도가 높은 것 같아 보인다. 이를 해결하기 위해 정규표현식으로 다시 토큰화하자.  \n",
    "이를 위해 먼저 raw()를 이용해 원문을 가져와서 documents를 만들고, 이에 대해 토큰화를 한다. 이왕 하는 김에 NLTK가 제공하는 불용어 사전을 이용해 불용어도 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7851af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of features: 48\n",
      "count of ' ': 825, count of 'e': 382, count of 't': 327, count of 'i': 234, count of 'a': 224, count of 'o': 216, count of 'n': 195, count of 's': 192, count of 'h': 172, count of 'r': 166, "
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\") # 정규표현식으로 토크나이저를 정의\n",
    "english_stops = set(stopwords.words('english')) # 영어 불용어를 가져옴\n",
    "\n",
    "# words() 대신 raw()로 원문을 가져옴\n",
    "documents = [movie_reviews.raw(fileid)]\n",
    "\n",
    "# stopwords의 적용과 토큰화를 동시에 수행.\n",
    "tokens = [[token for token in tokenizer.tokenize(doc) if token not in english_stops] for doc in documents]\n",
    "\n",
    "word_count = {}\n",
    "for text in documents:\n",
    "    for word in text:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "        \n",
    "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
    "\n",
    "print('num of features:', len(sorted_features))\n",
    "for word in sorted_features[:10]:\n",
    "    print(f\"count of '{word}': {word_count[word]}\", end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00e9a2",
   "metadata": {},
   "source": [
    "상위 빈도수를 가지는 단어 천 개만 추출해서 최종적으로 문서를 표현할 특성으로 사용하기로 한다.이때 중요한 것은, 특성 집합에는 단어의 순서가 있으며 이 순서에 따라 문서의 카운트 벡터 값이 결정된다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20684535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도가 높은 상위 1000개의 단어만 추출해 features를 구성\n",
    "word_features = sorted_features[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710cd16",
   "metadata": {},
   "source": [
    "이제 주어진 문서를 특성 벡터, 즉 카운트 벡터로 변환하는 함수를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9902d",
   "metadata": {},
   "source": [
    "함수가 제대로 작동하는지 알아보기 위해 아래와 같이 입, 출력의 예를 만들어 테스트를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ead7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 document를 feature로 변환하는 함수, word_features를 사용\n",
    "def document_features(document, word_features):\n",
    "    word_count = {}\n",
    "    for word in document: # document에 있는 단어들에 대해 빈도수를 먼저 계산\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "        \n",
    "    features = []\n",
    "    # word_features의 단어에 대해 계산된 빈도수를 features에 추가\n",
    "    for word in word_features:\n",
    "        features.append(word_count.get(word, 0)) # 빈도가 없는 단어는 0을 입력\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb993dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "word_features_ex = ['one', 'two', 'teen', 'couples', 'solo']\n",
    "doc_ex = ['two', 'two', 'couples']\n",
    "\n",
    "print(document_features(doc_ex, word_features_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5088a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( , 0), (e, 0), (t, 0), (i, 0), (a, 0), (o, 0), (n, 0), (s, 0), (h, 0), (r, 0), (l, 0), (d, 0), (m, 0), (u, 0), (g, 0), (c, 0), (w, 0), (f, 0), (y, 0), (p, 0), "
     ]
    }
   ],
   "source": [
    "feature_set = [document_features(d, word_features) for d in tokens]\n",
    "\n",
    "# 첫째 feature set의 내용을 앞 20개만 word_features의 단어와 함께 출력\n",
    "for i in range(20):\n",
    "    print(f'({word_features[i]}, {feature_set[0][i]})', end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310fcd5",
   "metadata": {},
   "source": [
    "## 사이킷런으로 카운트 벡터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ded081",
   "metadata": {},
   "source": [
    "사이킷런은 자체적인 토크나이저를 지원하므로, 사용자가 별도로 미리 토큰화를 하지 않아도 된다.  \n",
    "그러나 좀 더 세부적인 조정을 통해 성능을 높이고 싶을 때는 토크나이저를 함수로 정의하고 사이킷런에서 이를 사용하게 할 수 있다.  \n",
    "한글의 경우에는 KoNLPy로 형태소 분석을 수행해야 하므로 반드시 별도의 토크나이저를 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe9dbf8",
   "metadata": {},
   "source": [
    "이 절에서는 위에서 사용한 movie_reviews에 대해 사이킷런의 CountVector 클래스를 이용해 카운트 벡터를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8ae311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 준비, movie_reviews.raw()를 사용해 raw text를 추출\n",
    "reviews = [movie_reviews.raw(fileld) for fileld in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f52997",
   "metadata": {},
   "source": [
    "CountVectorizer 객체를 생성할 때 vocabulary 매개변수를 쓰면 앞에서 만든 word_features에 있는 단어들만으로 벡터를 구성할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f02e28e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(vocabulary=[' ', 'e', 't', 'i', 'a', 'o', 'n', 's', 'h', 'r',\n",
      "                            'l', 'd', 'm', 'u', 'g', 'c', 'w', 'f', 'y', 'p',\n",
      "                            ',', 'b', 'v', '\\n', '.', 'k', \"'\", '0', '\"', '-', ...])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# cv = CountVectorizer() # 모든 매개변수에 기본값을 사용하는 경우\n",
    "\n",
    "# 앞에서 생성한 word_features로 특성 집합을 지정하는 경우\n",
    "cv = CountVectorizer(vocabulary=word_features)\n",
    "\n",
    "# 특성 집합을 지정하지 않고 최대 특성의 수를 지정하는 경우\n",
    "# cv = CountVectorizer(max_feautures=1000)\n",
    "\n",
    "print(cv) # 객체의 인수를 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e07d2",
   "metadata": {},
   "source": [
    "객체를 생성했다면 아래와 같이 fit_transform()으로 특성 집합을 생성하고 카운트 벡터를 생성한다.  \n",
    "get_feature_names_out()를 출력하면 word_features와 사용된 단어 및 순서가 동일한 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d491458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ' 'e' 't' 'i' 'a' 'o' 'n' 's' 'h' 'r' 'l' 'd' 'm' 'u' 'g' 'c' 'w' 'f'\n",
      " 'y' 'p']\n",
      "[' ', 'e', 't', 'i', 'a', 'o', 'n', 's', 'h', 'r', 'l', 'd', 'm', 'u', 'g', 'c', 'w', 'f', 'y', 'p']\n"
     ]
    }
   ],
   "source": [
    "reviews_cv = cv.fit_transform(reviews) # reviews를 이용해 count vector를 학습하고, 변환\n",
    "print(cv.get_feature_names_out()[:20]) # count vector에 사용된 feature 이름을 반환\n",
    "print(word_features[:20]) # 비교를 위해 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f342c41",
   "metadata": {},
   "source": [
    "## 한국어 텍스트의 카운트 벡터 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b868cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c9877",
   "metadata": {},
   "source": [
    "여기서는 review 항목으로부터 카운트 벡터를 생성하는 것에 집중한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144862c0",
   "metadata": {},
   "source": [
    "먼저 영어 문서분류와 동일하게 자체 토크나이저를 사용해서 카운트 벡터를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b89a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10점' '18' '1987' '1도' '1점' '1점도' '2시간' '2시간이' '2편' '5점' '6점' '7점' '8점'\n",
      " 'cg' 'cg가' 'cg는' 'cg도' 'cg만' 'good' 'of' 'ㅋㅋ' 'ㅋㅋㅋ' 'ㅋㅋㅋㅋ' 'ㅎㅎ' 'ㅎㅎㅎ'\n",
      " 'ㅜㅜ' 'ㅠㅠ' 'ㅠㅠㅠ' 'ㅡㅡ' '가는' '가는줄' '가면' '가서' '가슴' '가슴아픈' '가슴이' '가장' '가족'\n",
      " '가족과' '가족들과' '가족의' '가족이' '가지고' '간만에' '갈수록' '감독' '감독님' '감독은' '감독의' '감독이'\n",
      " '감동' '감동과' '감동도' '감동은' '감동을' '감동이' '감동입니다' '감동적' '감동적이고' '감동적인' '감사드립니다'\n",
      " '감사합니다' '감정이' '갑자기' '갔는데' '갔다가' '강철비' '강추' '강추합니다' '같고' '같네요' '같다' '같습니다'\n",
      " '같아' '같아요' '같은' '같은데' '같음' '같이' '개연성' '개연성이' '개인적으로' '거의' '겁나' '것도' '것은'\n",
      " '것을' '것이' '것이다' '겨울왕국' '결국' '결말' '결말이' '계속' '고맙습니다' '곤지암' '공포' '공포를'\n",
      " '공포영화' '관객']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "daum_cv = CountVectorizer(max_features=1000)\n",
    "\n",
    "# review를 이용해 count vector를 학습하고 반환\n",
    "daum_DTM = daum_cv.fit_transform(df.review)\n",
    "\n",
    "print(daum_cv.get_feature_names_out()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986a587",
   "metadata": {},
   "source": [
    "위 결과는 문제가 많아보인다. 'cg'가 들어간 단어들이 전부 별도의 단어로 분류됐으며, '감동'이 들어간 단어는 더 심하다. 역시 KoNLPy의 형태소 분석기를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e754431c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#전체 형태소 결과: ['몰입', '할수밖에', '없다', '.', '어렵게', '생각', '할', '필요없다', '.', '내', '가', '전투', '에', '참여', '한', '듯', '손', '에', '땀', '이남', '.']\n",
      "#명사만 추출: ['몰입', '생각', '내', '전투', '참여', '듯', '손', '땀', '이남']\n",
      "#품사 태깅 결과: [('몰입', 'Noun'), ('할수밖에', 'Verb'), ('없다', 'Adjective'), ('.', 'Punctuation'), ('어렵게', 'Adjective'), ('생각', 'Noun'), ('할', 'Verb'), ('필요없다', 'Adjective'), ('.', 'Punctuation'), ('내', 'Noun'), ('가', 'Josa'), ('전투', 'Noun'), ('에', 'Josa'), ('참여', 'Noun'), ('한', 'Determiner'), ('듯', 'Noun'), ('손', 'Noun'), ('에', 'Josa'), ('땀', 'Noun'), ('이남', 'Noun'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "twitter_tag = Okt()\n",
    "\n",
    "print('#전체 형태소 결과:', twitter_tag.morphs(df.review[1]))\n",
    "print('#명사만 추출:', twitter_tag.nouns(df.review[1]))\n",
    "print('#품사 태깅 결과:', twitter_tag.pos(df.review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b5678",
   "metadata": {},
   "source": [
    "위 결과를 보면 전체 형태소를 다 사용하는 경우에는 필요없는 단어가 많아보이고, 명사만 쓰면 또 너무 적어보인다.  \n",
    "여기서는 명사, 동사, 형용사 세 개를 선택하기로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0ddd1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나만의 토크나이저 결과: ['몰입', '할수밖에', '없다', '어렵게', '생각', '할', '필요없다', '내', '전투', '참여', '듯', '손', '땀', '이남']\n"
     ]
    }
   ],
   "source": [
    "def my_tokenizer(doc):\n",
    "    return [\n",
    "        token\n",
    "        for token, pos in twitter_tag.pos(doc)\n",
    "        if pos in ['Noun', 'Verb', 'Adjective']\n",
    "    ]\n",
    "\n",
    "print('나만의 토크나이저 결과:', my_tokenizer(df.review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47793285",
   "metadata": {},
   "source": [
    "아래와 같이 tokenizer 매개변수에 my_tokenizer 함수를 지정해 객체를 생성한 후에, fit_transform으로 카운트 벡터를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c342485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가' '가는' '가는줄' '가면' '가서' '가슴' '가장' '가족' '가족영화' '가지' '가치' '각색' '간' '간다'\n",
      " '간만' '갈' '갈수록' '감' '감독' '감동' '감사' '감사합니다' '감상' '감성' '감정' '감탄' '갑자기' '갔는데'\n",
      " '갔다' '갔다가' '강' '강철' '강추' '같고' '같네요' '같다' '같습니다' '같아' '같아요' '같은' '같은데'\n",
      " '같음' '개' '개그' '개봉' '개연' '개인' '거' '거기' '거리' '거의' '걱정' '건' '건가' '건지' '걸'\n",
      " '겁니다' '것' '게' '겨울왕국' '결론' '결말' '경찰' '경험' '계속' '고' '고맙습니다' '고민' '고생' '곤지암'\n",
      " '곳' '공감' '공포' '공포영화' '과' '과거' '관' '관객' '관객수' '관람' '광주' '괜찮은' '교훈' '구성'\n",
      " '국내' '국민' '군인' '군함도' '굿' '권선' '귀신' '그' '그것' '그게' '그날' '그냥' '그닥' '그대로'\n",
      " '그때' '그래픽']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 토크나이저와 특성의 최대 개수를 지정\n",
    "# 명사만 추출하고 싶은 경우에는 tokenizer에 'twitter_tag.nouns'를 바로 지정해도 됨\n",
    "daum_cv = CountVectorizer(max_features=1000, tokenizer=my_tokenizer)\n",
    "\n",
    "# review를 이용해 count vector를 학습하고, 변환\n",
    "daum_DTM = daum_cv.fit_transform(df.review)\n",
    "\n",
    "print(daum_cv.get_feature_names_out()[:100]) # count vector에 사용된 feature 이름을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb57d93",
   "metadata": {},
   "source": [
    "## 카운트 벡터의 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e91b3",
   "metadata": {},
   "source": [
    "다음 예에서는 NLTK 영화 리뷰 문서들에 대해 유사도를 측정해본다.  \n",
    "제대로 유사도가 계산되는지 확인하기 위해, 먼저 유사도를 측정할 대상 문서를 만든다. 아래 예에서는 첫째 리뷰의 뒷부분 절반을 잘라서 대상 문서를 만들었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd189670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#대상 특성 행렬의 크기: (1, 48)\n",
      "#유사도 계산 행렬의 크기: (1, 2000)\n",
      "#유사도 계산결과를 역순으로 정렬: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 첫째 리뷰의 문자수를 확인하고 뒤 절반을 가져오기 위해 중심점을 찾음\n",
    "start = len(reviews[0]) // 2\n",
    "\n",
    "# 중심점으로부터 뒤 절반을 가져와서 비교할 문서를 생성\n",
    "source = reviews[0][-start:]\n",
    "\n",
    "# 코사인 유사도는 카운트 벡터에 대해 계산하므로 벡터로 변환\n",
    "# transform은 반드시 리스트나 행렬 형태의 입력을 요구하므로 리스트로 만들어서 입력\n",
    "source_cv = cv.transform([source])\n",
    "\n",
    "# 행렬의 크기를 확인, 문서가 하나이므로 (1, 1000)\n",
    "print('#대상 특성 행렬의 크기:', source_cv.shape)\n",
    "\n",
    "# 변환된 count vector와 기존 값들과의 similarity 계산\n",
    "sim_result = cosine_similarity(source_cv, reviews_cv)\n",
    "\n",
    "print('#유사도 계산 행렬의 크기:', sim_result.shape)\n",
    "print('#유사도 계산결과를 역순으로 정렬:', sorted(sim_result[0], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73da0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
